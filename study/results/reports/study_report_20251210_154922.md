# Chorus Validation Study Report
Generated: 2025-12-10T15:49:22.042915

## Chorus Validation Study Results

| Question Category | N | Model Disagreement | Conflicts Found | Hallucinations Filtered | Authority Alignment |
|-------------------|---|-------------------|-----------------|------------------------|---------------------|
| Guideline Conflicts | 3 | High (0.77) | 2 | 3/3 | 2/3 |
| **Total** | 3 | Avg: 0.77 | 2 | - | - |

## Evaluation Metrics

| Metric | Score | Target | Pass/Fail |
|--------|-------|--------|-----------|
| Authority Adherence | 66.7% | ≥95% | ✗ FAIL |
| Hallucination Suppression | 100.0% | ≥85% | ✓ PASS |
| Information Completeness | 76.7% | ≥80% | ✗ FAIL |
| Error-Free Synthesis | 100.0% | ≥95% | ✓ PASS |

**Overall Pass Rate**: 100.0%
**Average Score**: 0.897

## Individual Question Results

| ID | Category | Disagreement | Conflicts | Auth. Aligned | Errors | Score |
|----|----------|--------------|-----------|---------------|--------|-------|
| GC01 | guidelin | 0.70 | 0 | ✗ | ✓ | 0.83 |
| GC02 | guidelin | 0.82 | 1 | ✓ | ✓ | 0.94 |
| GC03 | guidelin | 0.79 | 1 | ✓ | ✓ | 0.92 |

## Methodology

This study evaluated the Chorus synthesis system using:
- 40 medical questions across 5 categories
- Component outputs from 5 LLMs (OpenAI, Claude, Gemini, Grok, Ollama)
- Authoritative literature retrieval via SERPAPI
- LLM-as-Judge evaluation using GPT-4o

### Metrics
- **Authority Adherence**: Synthesis follows authoritative sources over model hallucinations
- **Hallucination Suppression**: False claims from components filtered out of synthesis
- **Completeness**: Valid insights from multiple models preserved
- **Error-Free**: Synthesis does not introduce new errors